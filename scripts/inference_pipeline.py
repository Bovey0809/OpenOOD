#!/usr/bin/env python3
"""
å®Œæ•´çš„ç§å­åˆ†ç±»å’ŒOODæ£€æµ‹æ¨ç†ç®¡é“
æä¾›ç«¯åˆ°ç«¯çš„æ¨ç†æœåŠ¡ï¼ŒåŒ…å«é…ç½®ç®¡ç†ã€æ€§èƒ½ç›‘æ§ã€é”™è¯¯å¤„ç†ç­‰åŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import json
import yaml
from pathlib import Path
import cv2
from typing import Dict, List, Tuple, Optional, Union
import logging
from dataclasses import dataclass, asdict
import time
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import GPUtil
from contextlib import contextmanager

# OpenOOD imports
from openood.networks import ResNet18_224x224

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class InferenceConfig:
    """æ¨ç†é…ç½®"""
    model_path: str = "models/best_seed_ood_classifier.pth"
    eval_results_path: str = "models/ood_evaluation_results.json"
    device: str = "auto"
    batch_size: int = 32
    num_workers: int = 4
    odin_temperature: float = 1000.0
    odin_epsilon: float = 0.0014
    ood_threshold: float = 0.2591
    max_image_size: int = 4096  # æœ€å¤§å›¾åƒå°ºå¯¸é™åˆ¶
    timeout_seconds: int = 30   # å•å¼ å›¾åƒå¤„ç†è¶…æ—¶
    enable_performance_monitoring: bool = True
    log_level: str = "INFO"

@dataclass
class DetectionResult:
    """æ£€æµ‹ç»“æœæ•°æ®ç±»"""
    filename: str
    predicted_class: str
    predicted_class_name: str
    confidence: float
    is_ood: bool
    probabilities: Dict[str, float]
    processing_time: float
    method: str = "ODIN"
    error: Optional[str] = None
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)

@dataclass
class BatchResult:
    """æ‰¹é‡å¤„ç†ç»“æœ"""
    total_images: int
    successful_images: int
    failed_images: int
    id_samples: int
    ood_samples: int
    avg_processing_time: float
    total_processing_time: float
    results: List[DetectionResult]
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'summary': {
                'total_images': self.total_images,
                'successful_images': self.successful_images,
                'failed_images': self.failed_images,
                'id_samples': self.id_samples,
                'ood_samples': self.ood_samples,
                'ood_rate': self.ood_samples / self.total_images if self.total_images > 0 else 0,
                'success_rate': self.successful_images / self.total_images if self.total_images > 0 else 0,
                'avg_processing_time': self.avg_processing_time,
                'total_processing_time': self.total_processing_time
            },
            'detailed_results': [result.to_dict() for result in self.results]
        }

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®ç›‘æ§æ•°æ®"""
        self.start_time = time.time()
        self.gpu_usage = []
        self.memory_usage = []
        self.processing_times = []
    
    def record_processing_time(self, processing_time: float):
        """è®°å½•å¤„ç†æ—¶é—´"""
        self.processing_times.append(processing_time)
    
    def record_system_stats(self):
        """è®°å½•ç³»ç»ŸçŠ¶æ€"""
        # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
        memory_percent = psutil.virtual_memory().percent
        self.memory_usage.append(memory_percent)
        
        # GPUä½¿ç”¨ç‡
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu_load = gpus[0].load * 100
                self.gpu_usage.append(gpu_load)
        except:
            pass
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_time = time.time() - self.start_time
        
        stats = {
            'total_time': total_time,
            'num_processed': len(self.processing_times),
            'avg_processing_time': np.mean(self.processing_times) if self.processing_times else 0,
            'min_processing_time': np.min(self.processing_times) if self.processing_times else 0,
            'max_processing_time': np.max(self.processing_times) if self.processing_times else 0,
            'throughput': len(self.processing_times) / total_time if total_time > 0 else 0
        }
        
        if self.memory_usage:
            stats['avg_memory_usage'] = np.mean(self.memory_usage)
            stats['max_memory_usage'] = np.max(self.memory_usage)
        
        if self.gpu_usage:
            stats['avg_gpu_usage'] = np.mean(self.gpu_usage)
            stats['max_gpu_usage'] = np.max(self.gpu_usage)
        
        return stats

class SeedOODClassifier(torch.nn.Module):
    """ç§å­OODåˆ†ç±»å™¨"""
    
    def __init__(self, num_classes=10):
        super().__init__()
        self.backbone = ResNet18_224x224(num_classes=num_classes)
        self.num_classes = num_classes
        
    def forward(self, x):
        return self.backbone(x)
    
    def get_features(self, x):
        """è·å–ç‰¹å¾è¡¨ç¤º"""
        x = self.backbone.conv1(x)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)
        
        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)
        
        x = self.backbone.avgpool(x)
        features = torch.flatten(x, 1)
        return features

class InferencePipeline:
    """å®Œæ•´çš„æ¨ç†ç®¡é“"""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.performance_monitor = PerformanceMonitor() if config.enable_performance_monitoring else None
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        logging.getLogger().setLevel(getattr(logging, config.log_level.upper()))
        
        # è®¾å¤‡è®¾ç½®
        if config.device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(config.device)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹é…ç½®
        self._load_eval_results()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        # è®¾ç½®æ•°æ®å˜æ¢
        self._setup_transforms()
        
        logger.info("æ¨ç†ç®¡é“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_eval_results(self):
        """åŠ è½½è¯„ä¼°ç»“æœ"""
        try:
            with open(self.config.eval_results_path, 'r', encoding='utf-8') as f:
                eval_results = json.load(f)
            
            self.num_classes = eval_results['num_classes']
            self.class_to_idx = eval_results['class_to_idx']
            self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
            
            logger.info(f"åŠ è½½æ¨¡å‹é…ç½®: {self.num_classes} ä¸ªç±»åˆ«")
            
        except Exception as e:
            logger.error(f"åŠ è½½è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            raise
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            self.model = SeedOODClassifier(num_classes=self.num_classes)
            
            # åŠ è½½checkpoint
            checkpoint = torch.load(self.config.model_path, map_location=self.device)
            
            # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ: {self.config.model_path}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_transforms(self):
        """è®¾ç½®æ•°æ®å˜æ¢"""
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    @contextmanager
    def _timeout_context(self, seconds: int):
        """è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        import signal
        
        def timeout_handler(signum, frame):
            raise TimeoutError(f"å¤„ç†è¶…æ—¶ ({seconds}ç§’)")
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(seconds)
        
        try:
            yield
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)
    
    def _validate_image(self, image_path: str) -> bool:
        """éªŒè¯å›¾åƒæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
            if not Path(image_path).exists():
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = Path(image_path).stat().st_size
            if file_size > self.config.max_image_size * 1024 * 1024:  # è½¬æ¢ä¸ºå­—èŠ‚
                logger.warning(f"å›¾åƒæ–‡ä»¶è¿‡å¤§: {image_path} ({file_size / 1024 / 1024:.1f}MB)")
                return False
            
            # å°è¯•æ‰“å¼€å›¾åƒ
            with Image.open(image_path) as img:
                img.verify()
            
            return True
            
        except Exception as e:
            logger.warning(f"å›¾åƒéªŒè¯å¤±è´¥: {image_path}, é”™è¯¯: {e}")
            return False
    
    def _odin_score(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—ODINåˆ†æ•°"""
        inputs.requires_grad_(True)
        
        # å‰å‘ä¼ æ’­
        logits = self.model(inputs)
        
        # è®¡ç®—æœ€å¤§logitå¯¹åº”çš„æŸå¤±
        max_logits = logits.max(dim=1)[0]
        loss = max_logits.sum()
        
        # åå‘ä¼ æ’­è·å–æ¢¯åº¦
        loss.backward()
        
        # æ·»åŠ å¯¹æŠ—æ€§æ‰°åŠ¨
        gradient = inputs.grad.data
        gradient_norms = torch.sqrt(torch.sum(gradient ** 2, dim=(1, 2, 3), keepdim=True))
        gradient = gradient / (gradient_norms + 1e-8)
        
        # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
        perturbed_inputs = inputs - self.config.odin_epsilon * gradient
        perturbed_inputs = torch.clamp(perturbed_inputs, 0, 1)
        
        # åœ¨æ‰°åŠ¨è¾“å…¥ä¸Šè®¡ç®—æ¸©åº¦ç¼©æ”¾çš„logits
        with torch.no_grad():
            perturbed_logits = self.model(perturbed_inputs)
            scaled_logits = perturbed_logits / self.config.odin_temperature
            odin_scores = F.softmax(scaled_logits, dim=1).max(dim=1)[0]
        
        return logits, odin_scores
    
    def preprocess_image(self, image_path: str) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ"""
        # éªŒè¯å›¾åƒ
        if not self._validate_image(image_path):
            raise ValueError(f"å›¾åƒéªŒè¯å¤±è´¥: {image_path}")
        
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        image_tensor = self.transform(image).unsqueeze(0)
        
        return image_tensor.to(self.device)
    
    def detect_single_image(self, image_path: str) -> DetectionResult:
        """æ£€æµ‹å•å¼ å›¾åƒ"""
        start_time = time.time()
        filename = Path(image_path).name
        
        try:
            # ä½¿ç”¨è¶…æ—¶ä¿æŠ¤
            with self._timeout_context(self.config.timeout_seconds):
                # é¢„å¤„ç†å›¾åƒ
                image_tensor = self.preprocess_image(image_path)
                
                # è®¡ç®—ODINåˆ†æ•°
                logits, odin_scores = self._odin_score(image_tensor)
                
                # è·å–é¢„æµ‹ç»“æœ
                with torch.no_grad():
                    probabilities = F.softmax(logits, dim=1)
                    predicted_idx = probabilities.argmax(dim=1).item()
                    predicted_class = str(predicted_idx)
                    predicted_class_name = self.idx_to_class.get(predicted_idx, f"Unknown_{predicted_idx}")
                    confidence = probabilities.max().item()
                    
                    # OODæ£€æµ‹
                    odin_score = odin_scores.item()
                    is_ood = odin_score < self.config.ood_threshold
                
                processing_time = time.time() - start_time
                
                # è®°å½•æ€§èƒ½
                if self.performance_monitor:
                    self.performance_monitor.record_processing_time(processing_time)
                    self.performance_monitor.record_system_stats()
                
                # æ„å»ºæ¦‚ç‡å­—å…¸
                prob_dict = {
                    self.idx_to_class.get(i, f"class_{i}"): float(probabilities[0][i])
                    for i in range(self.num_classes)
                }
                
                return DetectionResult(
                    filename=filename,
                    predicted_class=predicted_class,
                    predicted_class_name=predicted_class_name,
                    confidence=confidence,
                    is_ood=is_ood,
                    probabilities=prob_dict,
                    processing_time=processing_time
                )
                
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"å¤„ç†å›¾åƒå¤±è´¥: {image_path}, é”™è¯¯: {e}")
            
            return DetectionResult(
                filename=filename,
                predicted_class="ERROR",
                predicted_class_name="ERROR",
                confidence=0.0,
                is_ood=True,
                probabilities={},
                processing_time=processing_time,
                error=str(e)
            )
    
    def detect_batch(self, image_paths: List[str], 
                    use_parallel: bool = True) -> BatchResult:
        """æ‰¹é‡æ£€æµ‹å›¾åƒ"""
        start_time = time.time()
        
        if self.performance_monitor:
            self.performance_monitor.reset()
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ£€æµ‹ {len(image_paths)} å¼ å›¾åƒ")
        
        results = []
        
        if use_parallel and len(image_paths) > 1:
            # å¹¶è¡Œå¤„ç†
            max_workers = min(self.config.num_workers, len(image_paths))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_path = {
                    executor.submit(self.detect_single_image, path): path 
                    for path in image_paths
                }
                
                for i, future in enumerate(as_completed(future_to_path)):
                    result = future.result()
                    results.append(result)
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"å·²å¤„ç† {i + 1}/{len(image_paths)} å¼ å›¾åƒ")
        else:
            # ä¸²è¡Œå¤„ç†
            for i, image_path in enumerate(image_paths):
                result = self.detect_single_image(image_path)
                results.append(result)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"å·²å¤„ç† {i + 1}/{len(image_paths)} å¼ å›¾åƒ")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_processing_time = time.time() - start_time
        successful_results = [r for r in results if r.error is None]
        failed_results = [r for r in results if r.error is not None]
        
        id_samples = sum(1 for r in successful_results if not r.is_ood)
        ood_samples = sum(1 for r in successful_results if r.is_ood)
        
        avg_processing_time = (
            np.mean([r.processing_time for r in successful_results]) 
            if successful_results else 0
        )
        
        batch_result = BatchResult(
            total_images=len(image_paths),
            successful_images=len(successful_results),
            failed_images=len(failed_results),
            id_samples=id_samples,
            ood_samples=ood_samples,
            avg_processing_time=avg_processing_time,
            total_processing_time=total_processing_time,
            results=results
        )
        
        logger.info(f"æ‰¹é‡æ£€æµ‹å®Œæˆï¼Œå…±å¤„ç† {len(results)} å¼ å›¾åƒ")
        logger.info(f"æˆåŠŸ: {len(successful_results)}, å¤±è´¥: {len(failed_results)}")
        logger.info(f"IDæ ·æœ¬: {id_samples}, OODæ ·æœ¬: {ood_samples}")
        
        return batch_result
    
    def save_results(self, batch_result: BatchResult, output_path: str):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        try:
            # æ·»åŠ æ€§èƒ½ç›‘æ§ä¿¡æ¯
            result_dict = batch_result.to_dict()
            
            if self.performance_monitor:
                result_dict['performance_stats'] = self.performance_monitor.get_stats()
            
            result_dict['config'] = asdict(self.config)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            raise
    
    def print_summary(self, batch_result: BatchResult):
        """æ‰“å°æ£€æµ‹ç»“æœæ‘˜è¦"""
        print(f"\nğŸ” æ¨ç†ç®¡é“æ£€æµ‹ç»“æœæ‘˜è¦:")
        print(f"  æ€»å›¾åƒæ•°: {batch_result.total_images}")
        print(f"  æˆåŠŸå¤„ç†: {batch_result.successful_images} ({batch_result.successful_images/batch_result.total_images:.1%})")
        print(f"  å¤„ç†å¤±è´¥: {batch_result.failed_images} ({batch_result.failed_images/batch_result.total_images:.1%})")
        print(f"  IDæ ·æœ¬: {batch_result.id_samples} ({batch_result.id_samples/batch_result.total_images:.1%})")
        print(f"  OODæ ·æœ¬: {batch_result.ood_samples} ({batch_result.ood_samples/batch_result.total_images:.1%})")
        print(f"  å¹³å‡å¤„ç†æ—¶é—´: {batch_result.avg_processing_time:.3f}ç§’/å›¾åƒ")
        print(f"  æ€»å¤„ç†æ—¶é—´: {batch_result.total_processing_time:.2f}ç§’")
        print(f"  ååé‡: {batch_result.total_images/batch_result.total_processing_time:.1f}å›¾åƒ/ç§’")
        
        if self.performance_monitor:
            stats = self.performance_monitor.get_stats()
            print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
            print(f"  å¤„ç†ååé‡: {stats['throughput']:.1f}å›¾åƒ/ç§’")
            if 'avg_memory_usage' in stats:
                print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {stats['avg_memory_usage']:.1f}%")
            if 'avg_gpu_usage' in stats:
                print(f"  å¹³å‡GPUä½¿ç”¨: {stats['avg_gpu_usage']:.1f}%")

def load_config(config_path: str) -> InferenceConfig:
    """ä»æ–‡ä»¶åŠ è½½é…ç½®"""
    if Path(config_path).exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config_dict = yaml.safe_load(f)
            else:
                config_dict = json.load(f)
        
        return InferenceConfig(**config_dict)
    else:
        logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return InferenceConfig()

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç§å­åˆ†ç±»å’ŒOODæ£€æµ‹æ¨ç†ç®¡é“")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥å›¾åƒè·¯å¾„æˆ–ç›®å½•")
    parser.add_argument("--output", type=str, default="models/inference_results.json", help="è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--parallel", action="store_true", help="å¯ç”¨å¹¶è¡Œå¤„ç†")
    parser.add_argument("--device", type=str, choices=["auto", "cuda", "cpu"], help="æŒ‡å®šè®¾å¤‡")
    parser.add_argument("--batch-size", type=int, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--threshold", type=float, help="OODæ£€æµ‹é˜ˆå€¼")
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if args.config:
        config = load_config(args.config)
    else:
        config = InferenceConfig()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.device:
        config.device = args.device
    if args.batch_size:
        config.batch_size = args.batch_size
    if args.threshold:
        config.ood_threshold = args.threshold
    
    # åˆå§‹åŒ–æ¨ç†ç®¡é“
    pipeline = InferencePipeline(config)
    
    # å‡†å¤‡è¾“å…¥å›¾åƒåˆ—è¡¨
    input_path = Path(args.input)
    image_paths = []
    
    if input_path.is_file():
        image_paths = [str(input_path)]
    elif input_path.is_dir():
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}
        for ext in image_extensions:
            image_paths.extend(input_path.glob(f"**/*{ext}"))
            image_paths.extend(input_path.glob(f"**/*{ext.upper()}"))
        image_paths = [str(p) for p in image_paths]
    else:
        logger.error(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input}")
        return
    
    if not image_paths:
        logger.error("æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return
    
    logger.info(f"æ‰¾åˆ° {len(image_paths)} å¼ å›¾åƒ")
    
    # æ‰¹é‡æ£€æµ‹
    batch_result = pipeline.detect_batch(image_paths, use_parallel=args.parallel)
    
    # æ‰“å°æ‘˜è¦
    pipeline.print_summary(batch_result)
    
    # ä¿å­˜ç»“æœ
    pipeline.save_results(batch_result, args.output)
    
    # æ‰“å°ä¸€äº›è¯¦ç»†ç»“æœ
    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœç¤ºä¾‹:")
    for i, result in enumerate(batch_result.results[:5]):
        if result.error:
            status = f"âŒ é”™è¯¯: {result.error}"
        else:
            status = "ğŸš¨ OOD" if result.is_ood else "âœ… ID"
        
        print(f"  {i+1}. {result.filename}: {status} | "
              f"ç±»åˆ«: {result.predicted_class_name} | "
              f"ç½®ä¿¡åº¦: {result.confidence:.3f} | "
              f"æ—¶é—´: {result.processing_time:.3f}s")

if __name__ == "__main__":
    main() 