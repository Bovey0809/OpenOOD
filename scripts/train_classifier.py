#!/usr/bin/env python3
"""
ç§å­åˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨OpenOODæ¡†æ¶è®­ç»ƒ10ç±»ç§å­åˆ†ç±»æ¨¡å‹
"""

import os
import sys
import json
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt

# æ·»åŠ OpenOODè·¯å¾„
sys.path.append('.')

from openood.networks import get_network
from scripts.prepare_dataset import SeedDataset, get_transforms, create_dataloaders
from openood.utils import Config

class SeedClassifierTrainer:
    """ç§å­åˆ†ç±»å™¨è®­ç»ƒç±»"""
    
    def __init__(self, config_path=None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ•°æ®é›†ä¿¡æ¯
        self.load_dataset_info()
        
        # åˆ›å»ºæ¨¡å‹
        self.create_model()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.create_dataloaders()
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        self.setup_training()
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path("models/seed_classifier")
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
    def load_dataset_info(self):
        """åŠ è½½æ•°æ®é›†ä¿¡æ¯"""
        dataset_info_path = Path("datasets/seeds/dataset_info.json")
        if not dataset_info_path.exists():
            raise FileNotFoundError("æ•°æ®é›†ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ prepare_dataset.py")
        
        with open(dataset_info_path, 'r', encoding='utf-8') as f:
            self.dataset_info = json.load(f)
        
        self.num_classes = self.dataset_info['num_classes']
        print(f"æ•°æ®é›†ä¿¡æ¯åŠ è½½å®Œæˆï¼Œç±»åˆ«æ•°: {self.num_classes}")
        
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        # åˆ›å»ºç½‘ç»œé…ç½®
        class NetworkConfig:
            def __init__(self, num_classes):
                self.name = 'resnet18_224x224'
                self.num_classes = num_classes
                self.pretrained = False
                self.checkpoint = None
                self.num_gpus = 1
        
        network_config = NetworkConfig(self.num_classes)
        
        # è·å–ç½‘ç»œ
        self.model = get_network(network_config)
        self.model = self.model.to(self.device)
        
        print(f"æ¨¡å‹åˆ›å»ºå®Œæˆ: ResNet18_224x224")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def create_dataloaders(self):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        self.train_loader, self.val_loader, self.ood_loader = create_dataloaders(
            self.dataset_info, 
            batch_size=32, 
            image_size=224
        )
        
        print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(self.train_loader)} æ‰¹æ¬¡")
        print(f"  éªŒè¯é›†: {len(self.val_loader)} æ‰¹æ¬¡")
        print(f"  OODæµ‹è¯•é›†: {len(self.ood_loader)} æ‰¹æ¬¡")
        
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒå‚æ•°"""
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=0.001, 
            weight_decay=1e-4
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer, 
            step_size=10, 
            gamma=0.1
        )
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'lr': []
        }
        
        print("è®­ç»ƒå‚æ•°è®¾ç½®å®Œæˆ")
        
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1} [Train]')
        
        for batch_idx, (images, labels) in enumerate(pbar):
            images, labels = images.to(self.device), labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'Loss': f'{running_loss/(batch_idx+1):.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
        
        epoch_loss = running_loss / len(self.train_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch+1} [Val]')
            
            for batch_idx, (images, labels) in enumerate(pbar):
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f'{running_loss/(batch_idx+1):.4f}',
                    'Acc': f'{100.*correct/total:.2f}%'
                })
        
        epoch_loss = running_loss / len(self.val_loader)
        epoch_acc = 100. * correct / total
        
        return epoch_loss, epoch_acc
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'train_history': self.train_history,
            'dataset_info': self.dataset_info
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, self.save_dir / 'latest_checkpoint.pth')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, self.save_dir / 'best_model.pth')
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Epoch {epoch+1})")
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
        
        epochs = range(1, len(self.train_history['train_loss']) + 1)
        
        # æŸå¤±æ›²çº¿
        ax1.plot(epochs, self.train_history['train_loss'], 'b-', label='Train Loss')
        ax1.plot(epochs, self.train_history['val_loss'], 'r-', label='Val Loss')
        ax1.set_title('Loss Curves')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        ax2.plot(epochs, self.train_history['train_acc'], 'b-', label='Train Acc')
        ax2.plot(epochs, self.train_history['val_acc'], 'r-', label='Val Acc')
        ax2.set_title('Accuracy Curves')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        ax3.plot(epochs, self.train_history['lr'], 'g-', label='Learning Rate')
        ax3.set_title('Learning Rate')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('LR')
        ax3.legend()
        ax3.grid(True)
        ax3.set_yscale('log')
        
        # è®­ç»ƒæ€»ç»“
        ax4.text(0.1, 0.8, f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(self.train_history['val_acc']):.2f}%", 
                transform=ax4.transAxes, fontsize=12)
        ax4.text(0.1, 0.7, f"æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {self.train_history['train_acc'][-1]:.2f}%", 
                transform=ax4.transAxes, fontsize=12)
        ax4.text(0.1, 0.6, f"æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {self.train_history['val_acc'][-1]:.2f}%", 
                transform=ax4.transAxes, fontsize=12)
        ax4.text(0.1, 0.5, f"æ€»è®­ç»ƒè½®æ•°: {len(epochs)}", 
                transform=ax4.transAxes, fontsize=12)
        ax4.set_title('Training Summary')
        ax4.axis('off')
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'training_history.png', dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š è®­ç»ƒå†å²å›¾è¡¨å·²ä¿å­˜åˆ°: {self.save_dir / 'training_history.png'}")
    
    def train(self, num_epochs=50):
        """è®­ç»ƒæ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒç§å­åˆ†ç±»æ¨¡å‹ (å…± {num_epochs} è½®)")
        print("="*60)
        
        best_val_acc = 0.0
        start_time = time.time()
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)
            
            # éªŒè¯
            val_loss, val_acc = self.validate_epoch(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            self.train_history['lr'].append(current_lr)
            
            # æ‰“å°ç»“æœ
            print(f"Epoch {epoch+1:3d}/{num_epochs} | "
                  f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:6.2f}% | "
                  f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:6.2f}% | "
                  f"LR: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
            
            # æ¯5è½®ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
            if (epoch + 1) % 5 == 0 or is_best:
                self.save_checkpoint(epoch, is_best)
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        print("="*60)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        self.plot_training_history()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_checkpoint(num_epochs-1)
        
        return best_val_acc

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ± ç§å­åˆ†ç±»æ¨¡å‹è®­ç»ƒ")
    print("="*50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SeedClassifierTrainer()
    
    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train(num_epochs=30)
    
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {trainer.save_dir}")

if __name__ == "__main__":
    main() 