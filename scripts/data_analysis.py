#!/usr/bin/env python3
"""
ç§å­æ•°æ®é›†åˆ†æè„šæœ¬
åˆ†æè®­ç»ƒæ•°æ®å’Œç«å“ç§å­æ•°æ®çš„ç»“æ„ã€åˆ†å¸ƒå’Œç‰¹å¾
"""

import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
from collections import defaultdict
import pandas as pd

def analyze_image_properties(image_path):
    """åˆ†æå•å¼ å›¾åƒçš„å±æ€§"""
    try:
        img = cv2.imread(str(image_path))
        if img is None:
            return None
        
        height, width, channels = img.shape
        file_size = os.path.getsize(image_path) / (1024 * 1024)  # MB
        
        # è®¡ç®—å›¾åƒç»Ÿè®¡ä¿¡æ¯
        mean_bgr = np.mean(img, axis=(0, 1))
        std_bgr = np.std(img, axis=(0, 1))
        
        return {
            'path': str(image_path),
            'width': width,
            'height': height,
            'channels': channels,
            'file_size_mb': round(file_size, 2),
            'mean_b': round(mean_bgr[0], 2),
            'mean_g': round(mean_bgr[1], 2),
            'mean_r': round(mean_bgr[2], 2),
            'std_b': round(std_bgr[0], 2),
            'std_g': round(std_bgr[1], 2),
            'std_r': round(std_bgr[2], 2),
        }
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
        return None

def analyze_dataset():
    """åˆ†ææ•´ä¸ªæ•°æ®é›†"""
    project_root = Path(".")
    
    # åˆ†æè®­ç»ƒæ•°æ®
    train_images_dir = project_root / "train" / "images"
    train_mask_dir = project_root / "train" / "mask"
    
    # åˆ†æç«å“ç§å­æ•°æ®
    ood_dir = project_root / "ç«å“ç§å­"
    
    results = {
        'train_data': {
            'images': [],
            'masks': [],
            'total_images': 0,
            'total_masks': 0
        },
        'ood_data': {
            'categories': {},
            'total_images': 0
        },
        'summary': {}
    }
    
    print("=== åˆ†æè®­ç»ƒæ•°æ® ===")
    
    # åˆ†æè®­ç»ƒå›¾åƒ
    if train_images_dir.exists():
        image_files = list(train_images_dir.glob("*.jpg")) + list(train_images_dir.glob("*.JPG"))
        results['train_data']['total_images'] = len(image_files)
        
        print(f"æ‰¾åˆ° {len(image_files)} å¼ è®­ç»ƒå›¾åƒ")
        
        for i, img_path in enumerate(image_files[:10]):  # åˆ†æå‰10å¼ ä½œä¸ºæ ·æœ¬
            props = analyze_image_properties(img_path)
            if props:
                results['train_data']['images'].append(props)
            
            if i % 5 == 0:
                print(f"å·²åˆ†æ {i+1}/{min(10, len(image_files))} å¼ å›¾åƒ...")
    
    # åˆ†æè®­ç»ƒmask
    if train_mask_dir.exists():
        mask_files = list(train_mask_dir.glob("*.jpg")) + list(train_mask_dir.glob("*.JPG")) + \
                    list(train_mask_dir.glob("*.png")) + list(train_mask_dir.glob("*.PNG"))
        results['train_data']['total_masks'] = len(mask_files)
        print(f"æ‰¾åˆ° {len(mask_files)} å¼ maskå›¾åƒ")
    
    print("\n=== åˆ†æç«å“ç§å­æ•°æ®ï¼ˆOODæ•°æ®ï¼‰===")
    
    # åˆ†æç«å“ç§å­æ•°æ®
    if ood_dir.exists():
        for category_dir in ood_dir.iterdir():
            if category_dir.is_dir():
                category_name = category_dir.name
                image_files = list(category_dir.glob("*.jpg")) + list(category_dir.glob("*.JPG"))
                
                results['ood_data']['categories'][category_name] = {
                    'count': len(image_files),
                    'images': []
                }
                results['ood_data']['total_images'] += len(image_files)
                
                print(f"{category_name}: {len(image_files)} å¼ å›¾åƒ")
                
                # åˆ†ææ¯ä¸ªç±»åˆ«çš„å‰å‡ å¼ å›¾åƒ
                for img_path in image_files[:3]:
                    props = analyze_image_properties(img_path)
                    if props:
                        results['ood_data']['categories'][category_name]['images'].append(props)
    
    # ç”Ÿæˆæ€»ç»“
    results['summary'] = {
        'total_train_images': results['train_data']['total_images'],
        'total_train_masks': results['train_data']['total_masks'],
        'total_ood_images': results['ood_data']['total_images'],
        'ood_categories': len(results['ood_data']['categories']),
        'ood_category_names': list(results['ood_data']['categories'].keys())
    }
    
    return results

def create_data_summary_report(results):
    """åˆ›å»ºæ•°æ®æ‘˜è¦æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ç§å­æ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š æ•°æ®é›†æ¦‚è§ˆ:")
    print(f"  è®­ç»ƒå›¾åƒ: {results['summary']['total_train_images']} å¼ ")
    print(f"  è®­ç»ƒmask: {results['summary']['total_train_masks']} å¼ ")
    print(f"  OODå›¾åƒ: {results['summary']['total_ood_images']} å¼ ")
    print(f"  OODç±»åˆ«: {results['summary']['ood_categories']} ç§")
    
    print(f"\nğŸ·ï¸  OODç±»åˆ«åˆ†å¸ƒ:")
    for category, data in results['ood_data']['categories'].items():
        print(f"  {category}: {data['count']} å¼ ")
    
    # åˆ†æå›¾åƒå±æ€§
    if results['train_data']['images']:
        train_images = results['train_data']['images']
        
        print(f"\nğŸ“ è®­ç»ƒå›¾åƒå±æ€§åˆ†æ (åŸºäº {len(train_images)} å¼ æ ·æœ¬):")
        
        widths = [img['width'] for img in train_images]
        heights = [img['height'] for img in train_images]
        file_sizes = [img['file_size_mb'] for img in train_images]
        
        print(f"  å›¾åƒå°ºå¯¸:")
        print(f"    å®½åº¦: {min(widths)} - {max(widths)} (å¹³å‡: {np.mean(widths):.0f})")
        print(f"    é«˜åº¦: {min(heights)} - {max(heights)} (å¹³å‡: {np.mean(heights):.0f})")
        print(f"  æ–‡ä»¶å¤§å°: {min(file_sizes):.1f} - {max(file_sizes):.1f} MB (å¹³å‡: {np.mean(file_sizes):.1f} MB)")
        
        # é¢œè‰²ç»Ÿè®¡
        mean_r = np.mean([img['mean_r'] for img in train_images])
        mean_g = np.mean([img['mean_g'] for img in train_images])
        mean_b = np.mean([img['mean_b'] for img in train_images])
        
        print(f"  å¹³å‡é¢œè‰²å€¼ (BGR): ({mean_b:.1f}, {mean_g:.1f}, {mean_r:.1f})")
    
    print(f"\nğŸ’¡ æ•°æ®é›†ç‰¹ç‚¹:")
    print(f"  - è¿™æ˜¯ä¸€ä¸ªç§å­åˆ†ç±»å’ŒOODæ£€æµ‹æ•°æ®é›†")
    print(f"  - è®­ç»ƒæ•°æ®åŒ…å«10ç§å·²çŸ¥ç§å­ç±»å‹ï¼ˆéœ€è¦è¿›ä¸€æ­¥åˆ†ç±»æ ‡æ³¨ï¼‰")
    print(f"  - OODæ•°æ®åŒ…å«5ç§å¤–æ¥ç§å­ç±»å‹ï¼Œç”¨äºæµ‹è¯•å¼‚å¸¸æ£€æµ‹")
    print(f"  - å›¾åƒè´¨é‡è¾ƒé«˜ï¼Œæ–‡ä»¶å¤§å°çº¦4MBå·¦å³")
    
    print(f"\nâš ï¸  æ³¨æ„äº‹é¡¹:")
    print(f"  - è®­ç»ƒæ•°æ®éœ€è¦æŒ‰ç§å­ç±»å‹è¿›è¡Œåˆ†ç±»æ ‡æ³¨")
    print(f"  - å½“å‰è®­ç»ƒå›¾åƒéƒ½åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œéœ€è¦åˆ›å»ºç±»åˆ«å­æ–‡ä»¶å¤¹")
    print(f"  - å»ºè®®å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†")
    print(f"  - OODæ•°æ®å¯ä»¥ç›´æ¥ç”¨äºå¼‚å¸¸æ£€æµ‹æµ‹è¯•")

def save_results(results, output_path="scripts/dataset_analysis.json"):
    """ä¿å­˜åˆ†æç»“æœåˆ°JSONæ–‡ä»¶"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹åˆ†æç§å­æ•°æ®é›†...")
    
    # åˆ†ææ•°æ®é›†
    results = analyze_dataset()
    
    # åˆ›å»ºæŠ¥å‘Š
    create_data_summary_report(results)
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    print(f"\nâœ… æ•°æ®é›†åˆ†æå®Œæˆï¼")
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
    print(f"  1. å¯¹è®­ç»ƒå›¾åƒè¿›è¡Œç±»åˆ«æ ‡æ³¨ï¼ˆåˆ›å»º10ä¸ªç±»åˆ«æ–‡ä»¶å¤¹ï¼‰")
    print(f"  2. å®ç°æ•°æ®åŠ è½½å™¨å’Œé¢„å¤„ç†ç®¡é“")
    print(f"  3. è®¾è®¡æ•°æ®å¢å¼ºç­–ç•¥")
    print(f"  4. åˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†")

if __name__ == "__main__":
    main() 